module cnn;


use CommandLine: all;
use MathArray: all;
use Structures: all;
use StdIO: all;

export all;

#define MBI 1

#if 0
inline
float[*] averageOuter( float[+] array)
{
  return with {
            ([0] <= iv < take( [1], shape( array))) : array[iv];
         } : fold( +, genarray( drop([1], shape( array)), 0f)) / tof( shape( array)[0]);
}
#else
inline
float[*] averageOuter( float[+] array)
{
  return with {
            (. <= iv <= .) : sum( with { (. <= [i] <= .) : array[[i]++iv]; 
                                  } : genarray( take( [1], shape( array)), 0f));
         } : genarray( drop([1], shape( array)), 0f) / tof( shape( array)[0]);
}
#endif


inline
float average( float[*] array)
{
   return sum( array) / tof( prod( shape( array)));
}

inline
int MaxPos( float[.,.,.,.,.] output)
{
   n = shape( output)[0];
   max = output[[0,0,0,0,0]];
   res = 0;
   for( i=0; i<n; i++) 
      if( output[[i,0,0,0,0]] > max) {
        max = output[[i,0,0,0,0]];
        res = i;
      }
   return res;
}

//------------------------------------------------------------------------------
// Convolution function
//------------------------------------------------------------------------------

inline
float[*] MultiConv( float[*] in, float[*] weights, float[*] bias)
{
  shp_act_map = (shape(in) - take( -[dim(in)], shape(weights))) + 1;
  shp_maps = drop( -[dim(in)], shape(weights));
  return with {
           (. <= iv <= .) : Convolve( in, weights[iv]) + bias[iv];
         } : genarray( shp_maps, genarray( shp_act_map, 0f));
}


inline
float[*] Convolve( float[*] in, float[*] weights)
{
  shp = shape( in) - shape(weights) + 1;
  out = with {
          (. <= iv <= .) : sum( with {
                                  (. <= ov <= .) : weights[ov] * in[iv+ov];
                                } : genarray( shape(weights), 0f));
        } : genarray( shp, 0f);

   return out;
}


float[*], float[*], float[*]
BackMultiConv( float[*] d_out, float[*] weights, float[*] in, float[*] bias)
{
  shp_act_map = take( -[dim(in)], shape(weights));
  shp_maps = drop( -[dim(in)], shape(weights));
#ifdef MBI
  d_in = with {
            ( . <= iv <= .) :
                   with {
                      (0*shp_maps <= ov < shp_maps) {
                           lb = max( 0*shp_act_map, iv - take( -[dim(in)], shape(d_out)) + 1);
                           ub = min( shp_act_map, iv+1 );
                        } : with { 
                              ( lb <= ov2 < ub) : weights[ov ++ ov2] * d_out[ov ++ (iv-ov2)];
                            } : fold( +, 0f);
                   } : fold( +, 0f);
         } : genarray( shape(in), 0f);
#else
  d_in = with {
           (0*shp_maps <= iv < shp_maps) : BackIn2( d_out[iv], weights[iv], in);
         } : fold( +, genarray( shape( in), 0f));
#endif
  d_weights = with {
                (. <= iv <= .) : BackWeights2( d_out[iv], weights[iv], in);
              } : genarray( shp_maps, genarray( take( -[dim(in)], shape(weights)), 0f));
  d_bias = with {
             (. <= iv <= .) : BackBias( d_out[iv]);
           } : genarray( shp_maps, 0f);

  return ( d_in, d_weights, d_bias);
}

inline
float[*] BackIn( float[*] d_out, float[*] weights, float[*] in)
{
  return with {
            ( 0*shape( weights) <= ov < shape( weights)) :
                   with {
                    (ov <= iv < ov+shape(d_out)) : weights[ ov] * d_out[iv-ov];
                   } : genarray( shape( in), 0f );
         } : fold( +, genarray( shape(in), 0f));
}

inline
float[*] BackIn2( float[*] d_out, float[*] weights, float[*] in)
{
  return with {
            ( . <= iv <= .):
                   with {
                    ( max( 0*shape( weights), (iv - shape(d_out)+1)) <= ov < min( shape(weights), iv+1)) : weights[ ov] * d_out[iv-ov];
                   } : fold( +, 0f );
         } : genarray( shape(in), 0f);
}


inline
float[*] BackWeights( float[*] d_out, float[*] weights, float[*] in)
{
  return with {
           ( 0*shape( weights) <= iv < shape( d_out)) : 
                   with { 
                    (. <= ov <= .) : in[ iv+ov] * d_out[iv];
                   } : genarray( shape( weights), 0f );
              } : fold( +, genarray( shape(weights), 0f));
}

inline
float[*] BackWeights2( float[*] d_out, float[*] weights, float[*] in)
{
  return with {
           ( . <= ov <= .) :
                   with { 
                    (0*shape( d_out) <= iv < shape( d_out)) : in[ iv+ov] * d_out[iv];
                   } : fold( +, 0f );
              } : genarray( shape( weights), 0f);
}

inline
float[*] BackBias( float[*] d_out)
{
  return sum( d_out);
}

//------------------------------------------------------------------------------
// Activation functions
//------------------------------------------------------------------------------

inline
float[+] Logistic( float[+] in)
{
  return 1f/(1f + exp( -(in)));
}

inline
float[*] BackLogistic( float[*] d_out, float[*] out)
{
  return d_out * out * (1f - out);
}

//------------------------------------------------------------------------------
// Pooling functions
//------------------------------------------------------------------------------


inline
float[*] AveragePool( float[*] in, int[.] filter)
{
  ones = genarray( [dim( in)], 1);
  filter = drop( shape( filter), ones) ++ filter;
  shp = shape( in) / filter;
  /*
   * out = { iv -> average( { ov -> in[iv+ov] | ov < filter})
   *             | iv < shp};
   */
  out = with {
          (. <= iv <= .) : average( with {
                                      (. <= ov <= .) : in[iv*filter+ov];
                                    } : genarray( filter, 0f));
        } : genarray( shp, 0f);
  return out;
}

inline
float[*] BackAveragePool( float[*] d_out, int[.] filter )
{
  ones = genarray( [dim( d_out)], 1);
  filter = drop( shape( filter), ones) ++ filter;
  shp = shape( d_out) * filter;
  d_in = with {
           (. <= iv <=.) : d_out[iv/filter] / tof( prod( filter));
         } : genarray( shp, 0f);
  return d_in;
}

//------------------------------------------------------------------------------
// Error functions
//------------------------------------------------------------------------------

inline
float MeanSquaredError( float[*] result, float[*] labels)
{
  return sum ( 0.5f * ( labels - result) * ( labels - result) );
}


//------------------------------------------------------------------------------
// Commandline functions
//------------------------------------------------------------------------------

int, int, int, int, float
CnnReadParameters( int epochs, int batchsize, int trainings, int tests, float rate)
{
   if( (argc() == 2) && (strcmp( argv(1), "-h") == 0 )) {
      printf( "%s -mt <n> -e <epocs> -b <batchsize> -tr <training-items>"
              " -te <test-items> -r <rate>\n", argv(0));
      epochs = 0;
      batchsize = 0;
      trainings = 0;
      tests = 0;
      rate = 0f;
   } else {
      if( (argc() >1)  && ( strcmp( argv(1), "-mt") == 0 )) {
        off = 2;
      } else {
        off = 0;
      }
      while( argc() > off+1) {
         if( strcmp( argv(off+1), "-e") == 0 ) {
            epochs = toi( argv( off+2));
         } else if( strcmp( argv(off+1), "-b") == 0 ) {
            batchsize = toi( argv( off+2));
         } else if( strcmp( argv(off+1), "-tr") == 0 ) {
            trainings = toi( argv( off+2));
         } else if( strcmp( argv(off+1), "-te") == 0 ) {
            tests = toi( argv( off+2));
         } else if( strcmp( argv(off+1), "-r") == 0 ) {
            rate = tof( argv( off+2));
         } else {
            printf( "ignoring non-recognised parameter %s!\n", argv(off+1));
            off --;
         }
         off += 2;
      }
   }

   return (epochs, batchsize, trainings, tests, rate);
}

