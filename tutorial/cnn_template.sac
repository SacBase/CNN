use Benchmarking: all;
use CommandLine: all;
use MathArray: all;
use Structures: all except {sum};
import Structures: {sum};
use StdIO: all;


int MaxPos( float[10,1,1,1,1] output)
{
   max = output[[0,0,0,0,0]];
   res = 0;
   for( i=0; i<10; i++) 
      if( output[[i,0,0,0,0]] > max) {
        max = output[[i,0,0,0,0]];
        res = i;
      }
   return res;
}

//------------------------------------------------------------------------------
// Convolution function
//------------------------------------------------------------------------------

inline
float[*] MultiConv( float[*] in, float[*] weights, float[*] bias)
{
  return { iv -> Convolve( in, weights[iv]) + bias[iv]
           | iv < drop( -[dim(in)], shape(weights)) };
}


inline
float[*] Convolve( float[*] in, float[*] w)
{
  out = { iv -> sum( { ov-> in[iv+ov] * w[ov]
                       | ov < shape(w) }) 
          | iv < shape(in)-shape(w)+1 };

   return out;
}


specialize float[*], float[*], float[*] BackMultiConv( float[6,24,24] d_out, float[6,5,5] weights, float[28,28] in, float[6] bias, float rate);
specialize float[*], float[*], float[*] BackMultiConv( float[12,1,8,8] d_out, float[12,6,5,5] weights, float[6,12,12] in, float[10] bias, float rate);
specialize float[*], float[*], float[*] BackMultiConv( float[10,1,1,1,1] d_out, float[10,12,1,4,4] weights, float[12,1,4,4] in, float[12] bias, float rate);

float[*], float[*], float[*]
BackMultiConv( float[*] d_out, float[*] weights, float[*] in, float[*] bias, float rate)
{
  shp_act_map = take( -[dim(in)], shape(weights));
  shp_maps = drop( -[dim(in)], shape(weights));
  d_in = { jv -> sum ({ kv -> { iv -> BackIn( d_out[iv], weights[iv], in) | iv <shp_maps} [kv++jv] 
                        | kv <shp_maps})
           | jv < shape(in)} ;
  d_weights = {iv -> BackWeights( d_out[iv], weights[iv], in) | iv < shp_maps};
  d_bias = {iv -> BackBias( d_out[iv]) | iv < shp_maps };

  return ( d_in, weights - rate*d_weights, bias - rate*d_bias);
}

specialize float[*] BackIn( float[24,24] d_out, float[5,5] weights, float[28,28] in);
specialize float[*] BackIn( float[1,8,8] d_out, float[6,5,5] weights, float[6,12,12] in);
specialize float[*] BackIn( float[1,1,1,1] d_out, float[12,1,4,4] weights, float[12,1,4,4] in);

float[*] BackIn( float[*] d_out, float[*] weights, float[*] in)
{
  return { iv -> sum( { ov -> weights[ ov] * d_out[iv-ov] | max( 0*shape( weights), (iv - shape(d_out)+1)) <= ov < min( shape(weights), iv+1) } )
           | iv < shape(in) };
}


specialize float[*] BackWeights( float[24,24] d_out, float[5,5] weights, float[28,28] in);
specialize float[*] BackWeights( float[1,8,8] d_out, float[6,5,5] weights, float[6,12,12] in);
specialize float[*] BackWeights( float[1,1,1,1] d_out, float[12,1,4,4] weights, float[12,1,4,4] in);

float[*] BackWeights( float[*] d_out, float[*] weights, float[*] in)
{
  return { ov -> sum( { iv-> in[iv+ov] * d_out[iv]
                        | iv < shape(d_out) }) 
          | ov < shape(weights) };
}

inline
float[*] BackBias( float[*] d_out)
{
  return sum( d_out);
}

//------------------------------------------------------------------------------
// Activation functions
//------------------------------------------------------------------------------

inline
float[*] Logistic( float[*] in)
{
  return 1f/(1f + exp( -(in)));
}

inline
float[*] BackLogistic( float[*] d_out, float[*] out)
{
  return d_out * out * (1f - out);
}

//------------------------------------------------------------------------------
// Pooling functions
//------------------------------------------------------------------------------


inline
float[*] AveragePool( float[*] in, int[.] filter)
{
  ones = genarray( [dim( in)], 1);
  filter = drop( shape( filter), ones) ++ filter;
  shp = shape( in) / filter;

   out = { iv -> sum( { ov -> in[iv+ov] | ov < filter} / tof( prod( filter)))
           | iv < shp};

  return out;
}

inline
float[*] BackAveragePool( float[*] d_out, int[.] filter )
{
  ones = genarray( [dim( d_out)], 1);
  filter = drop( shape( filter), ones) ++ filter;
  shp = shape( d_out) * filter;

  d_in = { iv -> d_out[iv/filter] / tof(prod( filter))
         | iv < shape(d_out) * filter };

  return d_in;
}

//------------------------------------------------------------------------------
// Error functions
//------------------------------------------------------------------------------

inline
float MeanSquaredError( float[*] result, float[*] labels)
{
  return sum ( 0.5f * ( labels - result) * ( labels - result) );
}


//------------------------------------------------------------------------------
// Network Construction
//------------------------------------------------------------------------------

float[6,5,5], float[6], float[12,6,5,5], float[12], float[10,12,1,4,4], float[10], float
TrainZhang( float[28,28] in, float[6,5,5] k1, float[6] b1,
                             float[12,6,5,5] k2, float[12] b2,
                             float[10,12,1,4,4] fc, float[10] b,
                             float[10,1,1,1,1] target, float rate)
{
  float[6,24,24] c1, d_c1;
  float[6,12,12] s1, d_s1;
  float[12,1,8,8] c2, d_c2;
  float[12,1,4,4] s2, d_s2;
  float[10,1,1,1,1] out, d_out;

  c1 = Logistic( MultiConv( in, k1, b1 ));
  s1 = AveragePool( c1, [2,2]);
  c2 = Logistic( MultiConv( s1, k2, b2));
  s2 = AveragePool( c2, [2,2]);
  out = Logistic( MultiConv( s2, fc, b));

  d_out = out - target;
  error = MeanSquaredError( out, target);

  d_s2, fc, b = BackMultiConv( BackLogistic( d_out, out), fc, s2, b, rate);
  d_c2 = BackAveragePool( d_s2, [2,2]);
  d_s1, k2, b2 = BackMultiConv( BackLogistic( d_c2, c2), k2, s1, b2, rate);
  d_c1 = BackAveragePool( d_s1, [2,2]);
  _, k1, b1 = BackMultiConv( BackLogistic( d_c1, c1), k1, in, b1, rate);

  return ( k1, b1, k2, b2, fc, b, error);
}

float[10,1,1,1,1]
TestZhang(float[28,28] in, float[6,5,5] k1, float[6] b1,
                             float[12,6,5,5] k2, float[12] b2,
                             float[10,12,1,4,4] fc, float[10] b )
{
   c1 = Logistic( MultiConv( in, k1, b1 ));
   s1 = AveragePool( c1, [2,2]);
   c2 = Logistic( MultiConv( s1, k2, b2));
   s2 = AveragePool( c2, [2,2]);
   out = Logistic( MultiConv( s2, fc, b));

   return out;
}

void RunZhang( int epocs, int training, int tests, float rate)
{
   k1 = genarray( [6,5,5], 1f/25f);
   b1 = genarray( [6], 1f/6f);
   k2 = genarray( [12,6,5,5], 1f/150f);
   b2 = genarray( [12], 1f/12f);
   fc = genarray( [10,12,1,4,4], 1f/192f);
   b = genarray( [10], 1f/10f);

   printf( "Reading training images ...\n");
   training_images = ( float[60000,28,28]) mnist::ReadImages( "train-images.idx3-ubyte");
   printf( "Read %d training images ...\n", shape(training_images)[0]);

   printf( "Reading training labels ...\n");
   training_labels = ( int[60000]) mnist::ReadLabels( "train-labels.idx1-ubyte");
   printf( "Read %d training labels ...\n", shape(training_labels)[0]);

   printf( "Reading test images ...\n");
   test_images = (float[10000,28,28])mnist::ReadImages( "t10k-images.idx3-ubyte");
   printf( "Read %d training images ...\n", shape(test_images)[0]);

   printf( "Reading test labels ...\n");
   test_labels = (int[10000])mnist::ReadLabels( "t10k-labels.idx1-ubyte");
   printf( "Read %d training labels ...\n", shape(test_labels)[0]);

   printf( "Running Zhang with %d epocs, %d training images, %d tests, and a rate of %f\n",
           epocs, training, tests, tod( rate));

   i1 = getInterval( "training", 0);
   i2 = getInterval( "test", 1);
   
   start( i1);
   for( epoc = 1; epoc <=epocs; epoc++) {
      error = 0d;
      for( i=1; i<= min( shape(training_images)[0], training); i++) { 
         in = training_images[i-1];
         target = genarray([10,1,1,1,1], 0f);
         target[[training_labels[i-1],0,0,0,0]] = 1f;
         k1, b1, k2, b2, fc, b, err = TrainZhang( in, k1, b1, k2, b2, fc, b, target, rate);

         error += tod(err);

         if( i%1000 == 0) {
            printf( "%d images trained\n", i);
         }

      }
      printf( "The mean error of epoc %d is %f\n",
              epoc,
              error / tod( min( shape(training_images)[0], training)) );
   }
   end( i1);
   printResult( i1);

   error = 0d;
   correct = 0;
   start( i2);
   for( i=0; i< min( shape(test_images)[0], tests); i++) {
      in = test_images[i];
      target = genarray([10,1,1,1,1], 0f);
      target[[test_labels[i],0,0,0,0]] = 1f;

      out = TestZhang( in, k1, b1, k2, b2, fc, b);

      if( MaxPos( out) == test_labels[i]) {
        correct ++;
      }
      error += tod( MeanSquaredError( out, target));  
   }
   end(i2);

   printf( "%d of %d numbers correctly identified!\n", correct, min( shape(test_images)[0], tests));
   printf( "The mean error of %d tests is %f\n",
           min( shape(test_images)[0], tests),
           error / tod( min( shape(test_images)[0], tests)) );
   printResult( i2);

}
  
//------------------------------------------------------------------------------

int main()
{
   if( (argc() >1)  && ( strcmp( argv(1), "-mt") == 0 )) {
     off = 2;
   } else {
     off = 0;
   }

   if( argc() > off+1) {
     epochs = toi( argv( off+1));
   } else {
     epochs = 20;
   }
   if( argc() > off+2) {
     training = toi( argv( off+2));
   } else {
     training = 1000;
   }
   if( argc() > off+3) {
     tests = toi( argv( off+3));
   } else {
     tests = 10000;
   }
   if( argc() > off+4) {
     rate = tof( argv( off+4));
   } else {
     rate = 0.5f;
   }

   RunZhang( epochs, training, tests, rate);

   return 0;
}

