use MathArray: all;
use Array: all except {sum};
import Array: {sum};
use StdIO: all;

inline
float[*] sum( int outer, float[*] array)
{
  return with {
           ( take( [outer], 0*shape(array)) <= iv < take( [outer], shape( array))): array[iv];
         } : fold( +, genarray( drop( [outer], shape( array)), 0f));
}

inline
float average( float[*] array)
{
   return sum( array) / tof( prod( shape( array)));
}

inline int[.] StripOnes( int[.] shp)
{
   num_ones = sum( toi( shp == 1));
   res = genarray( shape(shp) - num_ones, 0);
   idx = 0;
   for( i=0; i< shape(shp)[0]; i++) {
      if( shp[i] !=1) {
        res[idx] = shp[i];
        idx++;
      }
   }
   return res;
}

inline
int MaxPos( float[1,1,1,1,10] output)
{
   max = output[[0,0,0,0,0]];
   res = 0;
   for( i=0; i<10; i++) 
      if( output[[0,0,0,0,i]] > max) {
        max = output[[0,0,0,0,i]];
        res = i;
      }
   return res;
}

//------------------------------------------------------------------------------
// Convolution function
//------------------------------------------------------------------------------

inline
float[*] Conv( float[*] in, float[*] weights, float[*] bias)
{
  shp_act_map = (shape(in) - take( [dim(in)], shape(weights))) + 1;
  shp_maps = drop( [dim(in)], shape(weights));
  /*
   * { iv -> sum( dim(in), { ov -> weights[ov] * in[iv+ov]
   *                             | ov < take( [dim(in)], shape(weights)) } ) + bias
   *       | iv < shp_act_map }
   */
  out = with {
          (. <= iv <= .) : sum( dim(in),
                                with {
                                  (. <= ov <= .) : weights[ov] * in[iv+ov];
                                } : genarray( take( [dim(in)], shape(weights)), genarray( shp_maps, 0f)))
                           + bias;
        } : genarray( shp_act_map, genarray( shp_maps, 0f));
  
//  return reshape( StripOnes( shape( out)), out);
   return out;
}

inline
float[*], float[*], float[*]
BackConv( float[*] d_out, float[*] weights, float[*] in, float[*] bias, float rate)
{
  d_in = with {
              ( take( [dim(in)], 0*shape( weights)) <= ov < take( [dim(in)], shape( weights))) :
                   with {
                    (ov <= iv < ov+take([dim(in)], shape(d_out))) : sum( weights[ ov] * d_out[iv-ov]);
                   } : genarray( shape( in), 0f );
            } : fold( +, genarray( shape(in), 0f));
  d_weights = with {
                ( take( [dim(in)], 0*shape( d_out)) <= iv < take( [dim(in)], shape( d_out))) : 
                   with { 
                    (. <= ov <= .) : in[ iv+ov] * d_out[iv];
                   } : genarray( take( [dim(in)], shape( weights)),
                                 genarray( drop( [dim(in)], shape(weights)), 0f) );
              } : fold( +, genarray( shape(weights), 0f));
  d_bias = sum( dim(in), d_out);

  return ( d_in, weights - rate*d_weights, bias - rate*d_bias);
}

//------------------------------------------------------------------------------
// Activation functions
//------------------------------------------------------------------------------

inline
float[*] Logistic( float[*] in)
{
  return 1f/(1f + exp( -(in)));
}

inline
float[*] BackLogistic( float[*] d_out, float[*] out)
{
  return d_out * out * (1f - out);
}

//------------------------------------------------------------------------------
// Pooling functions
//------------------------------------------------------------------------------


inline
float[*] AveragePool( float[*] in, int[.] filter)
{
  ones = genarray( [dim( in)], 1);
  filter = filter ++ drop( shape( filter), ones);
  shp = shape( in) / filter;
  /*
   * out = { iv -> average( { ov -> in[iv+ov] | ov < filter})
   *             | iv < shp};
   */
  out = with {
          (. <= iv <= .) : average( with {
                                      (. <= ov <= .) : in[iv*filter+ov];
                                    } : genarray( filter, 0f));
        } : genarray( shp, 0f);
  return out;
}

inline
float[*] BackAveragePool( float[*] d_out, int[.] filter )
{
  ones = genarray( [dim( d_out)], 1);
  filter = filter ++ drop( shape( filter), ones);
  shp = shape( d_out) * filter;
  d_in = with {
           (. <= iv <=.) : d_out[iv/filter] / tof( prod( filter));
         } : genarray( shp, 0f);
  return d_in;
}

//------------------------------------------------------------------------------
// Error functions
//------------------------------------------------------------------------------

float MeanSquaredError( float[*] result, float[*] labels)
{
  return sum ( 0.5f * ( labels - result) * ( labels - result) );
}


//------------------------------------------------------------------------------
// Network Construction
//------------------------------------------------------------------------------

float[5,5,6], float[6], float[5,5,6,12], float[12], float[4,4,1,12,10], float[10], float
TrainZhang( float[28,28] in, float[5,5,6] k1, float[6] b1,
                             float[5,5,6,12] k2, float[12] b2,
                             float[4,4,1,12,10] fc, float[10] b,
                             float[1,1,1,1,10] target, float rate)
{
  // c1 = in |> Conv( k1, b1) |> Logistic() ;
  // s1 = c1 |> AveragePool( [2,2]);
  // c2 = s1 |> Conv( k2, b2) |> Logistic();
  // s2 = c2 |> AveragePool( [2,2]);
  // out = s2 |> Conv( fc, b) |> Logistic();

  c1 = Logistic( Conv( in, k1, b1 ));
  s1 = AveragePool( c1, [2,2]);
  c2 = Logistic( Conv( s1, k2, b2));
  s2 = AveragePool( c2, [2,2]);
  out = Logistic( Conv( s2, fc, b));

  d_out = out - target;
  error = MeanSquaredError( out, target);

  // d_s2, fc, b = d_out |> BackLogistic( out) |> BackConv( fc, s2, b, rate);
  // d_s1, k2, b2 = d_s2 |> BackAveragePool( [2,2]) |> BackLogistic( c2) |> BackConv( k2, s1, b2, rate);
  // _, k1, b1 = d_s1    |> BackAveragePool( [2,2]) |> BackLogistic( c1) |> BackConv( k1, in, b1, rate);

  d_s2, fc, b = BackConv( BackLogistic( d_out, out), fc, s2, b, rate);
  d_c2 = BackAveragePool( d_s2, [2,2]);
  d_s1, k2, b2 = BackConv( BackLogistic( d_c2, c2), k2, s1, b2, rate);
  d_c1 = BackAveragePool( d_s1, [2,2]);
  _, k1, b1 = BackConv( BackLogistic( d_c1, c1), k1, in, b1, rate);

  return ( k1, b1, k2, b2, fc, b, error);
}

float[1,1,1,1,10]
TestZhang(float[28,28] in, float[5,5,6] k1, float[6] b1,
                             float[5,5,6,12] k2, float[12] b2,
                             float[4,4,1,12,10] fc, float[10] b )
{
   c1 = Logistic( Conv( in, k1, b1 ));
   s1 = AveragePool( c1, [2,2]);
   c2 = Logistic( Conv( s1, k2, b2));
   s2 = AveragePool( c2, [2,2]);
   out = Logistic( Conv( s2, fc, b));

   return out;
}

void RunZhang()
{
   epocs = 2000;
   training = 500;
   tests = 10000;
   epocs_output = 1;
   rate = 0.5f;
   k1 = genarray( [5,5,6], 1f/25f);
   b1 = genarray( [6], 1f/6f);
   k2 = genarray( [5,5,6,12], 1f/150f);
   b2 = genarray( [12], 1f/12f);
   fc = genarray( [4,4,1,12,10], 1f/192f);
   b = genarray( [10], 1f/10f);

   printf( "Reading training images ...\n");
   training_images = ( float[60000,28,28]) mnist::ReadImages( "train-images.idx3-ubyte");
   printf( "Read %d training images ...\n", shape(training_images)[0]);

   printf( "Reading training labels ...\n");
   training_labels = ( int[60000]) mnist::ReadLabels( "train-labels.idx1-ubyte");
   printf( "Read %d training labels ...\n", shape(training_labels)[0]);

   printf( "Reading test images ...\n");
   test_images = (float[10000,28,28])mnist::ReadImages( "t10k-images.idx3-ubyte");
   printf( "Read %d training images ...\n", shape(test_images)[0]);

   printf( "Reading test labels ...\n");
   test_labels = (int[10000])mnist::ReadLabels( "t10k-labels.idx1-ubyte");
   printf( "Read %d training labels ...\n", shape(test_labels)[0]);

   printf( "Running Zhang with rate %f\n", tod( rate));

   
   for( epoc = 1; epoc <=epocs; epoc++) {
      error = 0d;
      for( i=1; i<= min( shape(training_images)[0], training); i++) { 
         in = training_images[i];
         target = genarray([1,1,1,1,10], 0f);
         target[[0,0,0,0,training_labels[i]]] = 1f;

         k1, b1, k2, b2, fc, b, err = TrainZhang( in, k1, b1, k2, b2, fc, b, target, rate);

         error += tod(err);

         if( i%1000 == 0) {
            printf( "%d images trained\n", i);
         }

      }
      printf( "The mean error of epoc %d is %f\n",
              epoc,
              error / tod( min( shape(training_images)[0], training)) );
   }

   error = 0d;
   correct = 0;
   for( i=0; i< min( shape(test_images)[0], tests); i++) {
      in = test_images[i];
      target = genarray([1,1,1,1,10], 0f);
      target[[0,0,0,0,test_labels[i]]] = 1f;

      out = TestZhang( in, k1, b1, k2, b2, fc, b);

      if( MaxPos( out) == test_labels[i]) {
        correct ++;
      }
      error += tod( MeanSquaredError( out, target));  
   }
   printf( "%d of %d numbers correctly identified!\n", correct, min( shape(test_images)[0], tests));
   printf( "The mean error of %d tests is %f\n",
           min( shape(test_images)[0], tests),
           error / tod( min( shape(test_images)[0], tests)) );

}
  
//------------------------------------------------------------------------------

void RunMazur()
{
  w14 = [[0.15f, 0.25f], [0.2f, 0.3f]];
  b1 = [0.35f, 0.35f];
  w58 = [[[0.4f, 0.5f], [0.45f, 0.55f]]];
  b2 = [0.6f, 0.6f];

  in = [0.05f, 0.1f];
  target = [[[0.01f, 0.99f]]];
  epocs = 10000;
  epocs_output = 1000;
  rate = 0.5f;

  printf( "Running Mazur with rate %f\n", tod( rate));

  for( i=0; i<epocs; i++) {
    // forward sweep:
    h = Logistic( Conv( in, w14, b1 ));
    o = Logistic( Conv( h, w58, b2 ));
   
    d_o = o - target;

    if( i % epocs_output == 0) {
      print( o);
      printf( "The error after %d epochs is %f\n", i, tod( MeanSquaredError( o, target)));
    }

    // back propagation:
    d_h, w58, b2 = BackConv( BackLogistic( d_o, o), w58, h, b2, rate);
    _, w14, b1 = BackConv( BackLogistic( d_h, h), w14, in, b1, rate);
  }

  h = Logistic( Conv( in, w14, b1 ));
  o = Logistic( Conv( h, w58, b2 ));
  print( o);
  printf( "The error after %d epochs is %f\n", i, tod( MeanSquaredError( o, target)));

}

//------------------------------------------------------------------------------

int main()
{
   RunZhang();

   return 0;
}

