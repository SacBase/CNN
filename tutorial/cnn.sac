use MathArray: all;
use Array: all except {sum};
import Array: {sum};
use StdIO: all;

inline
float[*] sum( int outer, float[*] array)
{
  return with {
           ( take( [outer], 0*shape(array)) <= iv < take( [outer], shape( array))): array[iv];
         } : fold( +, genarray( drop( [outer], shape( array)), 0f));
}

inline
float average( float[*] array)
{
   return sum( array) / tof( prod( shape( array)));
}

inline int[.] StripOnes( int[.] shp)
{
   num_ones = sum( toi( shp == 1));
   res = genarray( shape(shp) - num_ones, 0);
   idx = 0;
   for( i=0; i< shape(shp)[0]; i++) {
      if( shp[i] !=1) {
        res[idx] = shp[i];
        idx++;
      }
   }
   return res;
}

//------------------------------------------------------------------------------
// Convolution function
//------------------------------------------------------------------------------

inline
float[*] Conv( float[*] in, float[*] weights, float[*] bias)
{
  shp_act_map = (shape(in) - take( [dim(in)], shape(weights))) + 1;
  shp_maps = drop( [dim(in)], shape(weights));
  /*
   * { iv -> sum( dim(in), { ov -> weights[ov] * in[iv+ov]
   *                             | ov < take( [dim(in)], shape(weights)) } ) + bias
   *       | iv < shp_act_map }
   */
  out = with {
          (. <= iv <= .) : sum( dim(in),
                                with {
                                  (. <= ov <= .) : weights[ov] * in[iv+ov];
                                } : genarray( take( [dim(in)], shape(weights)), genarray( shp_maps, 0f)))
                           + bias;
        } : genarray( shp_act_map, genarray( shp_maps, 0f));
  
//  return reshape( StripOnes( shape( out)), out);
   return out;
}


inline
float[*], float[*], float[*]
BackConv( float[*] d_out, float[*] weights, float[*] in, float[*] bias, float rate)
{
  d_in = with {
              ( take( [dim(in)], 0*shape( weights)) <= ov < take( [dim(in)], shape( weights))) :
                   with {
                    (ov <= iv < ov+take([dim(in)], shape(d_out))) : sum( weights[ ov] * d_out[iv-ov]);
                   } : genarray( shape( in), 0f );
            } : fold( +, genarray( shape(in), 0f));
  d_weights = with {
                ( take( [dim(in)], 0*shape( d_out)) <= iv < take( [dim(in)], shape( d_out))) : 
                   with { 
                    (. <= ov <= .) : in[ iv+ov] * d_out[iv];
                   } : genarray( take( [dim(in)], shape( weights)),
                                 genarray( drop( [dim(in)], shape(weights)), 0f) );
              } : fold( +, genarray( shape(weights), 0f));
  d_bias = sum( dim(in), d_out);

  return ( d_in, weights - rate*d_weights, bias - rate*d_bias);
}

//------------------------------------------------------------------------------
// Activation functions
//------------------------------------------------------------------------------

inline
float[*] Logistic( float[*] in)
{
  return 1f/(1f + exp( -(in)));
}

inline
float[*] BackLogistic( float[*] d_out, float[*] out)
{
  return d_out * out * (1f - out);
}

//------------------------------------------------------------------------------
// Pooling functions
//------------------------------------------------------------------------------


inline
float[*] AveragePool( float[*] in, int[.] filter)
{
  ones = genarray( [dim( in)], 1);
  filter = filter ++ drop( shape( filter), ones);
  shp = shape( in) / filter;
  /*
   * out = { iv -> average( { ov -> in[iv+ov] | ov < filter})
   *             | iv < shp};
   */
  out = with {
          (. <= iv <= .) : average( with {
                                      (. <= ov <= .) : in[iv*filter+ov];
                                    } : genarray( filter, 0f));
        } : genarray( shp, 0f);
  return out;
}

inline
float[*] BackAveragePool( float[*] d_out, int[.] filter )
{
  ones = genarray( [dim( d_out)], 1);
  filter = filter ++ drop( shape( filter), ones);
  shp = shape( d_out) * filter;
  d_in = with {
           (. <= iv <=.) : d_out[iv/filter] / tof( prod( filter));
         } : genarray( shp, 0f);
  return d_in;
}

//------------------------------------------------------------------------------
// Error functions
//------------------------------------------------------------------------------

float MeanSquaredError( float[*] result, float[*] labels)
{
  return sum ( 0.5f * ( labels - result) * ( labels - result) );
}


//------------------------------------------------------------------------------
// Network Construction
//------------------------------------------------------------------------------

float[*] ZhangGetTrainingInput( int i)
{
   in = genarray( [28,28], 0f);
   in[6,6] = 42f;
   return in;
}

float[*] ZhangGetTargetOutput( int i)
{
   out = genarray( [1,1,1,1,10], 0f);
   return out;
}

void RunZhang()
{
   epocs = 10;
   epocs_output = 1;
   rate = 0.5f;
   num_train_ins = 1;
   num_test_ins = 0;
   k1 = genarray( [5,5,6], 1f/25f);
   b1 = genarray( [6], 1f/6f);
   k2 = genarray( [5,5,6,12], 1f/150f);
   b2 = genarray( [12], 1f/12f);
   fc = genarray( [4,4,1,12,10], 1f/192f);
   b = genarray( [10], 1f/10f);

   printf( "Running Zhang with rate %f\n", tod( rate));

   for( epoc = 0; epoc <epocs; epoc++) {
      for( i=0; i<num_train_ins; i++) { 
         in = ZhangGetTrainingInput( i);
         target = ZhangGetTargetOutput( i);

         // c1 = in |> Conv( k1, b1) |> Logistic() ;
         // s1 = c1 |> AveragePool( [2,2]);
         // c2 = s1 |> Conv( k2, b2) |> Logistic();
         // s2 = c2 |> AveragePool( [2,2]);
         // out = s2 |> Conv( fc, b) |> Logistic();

         c1 = Logistic( Conv( in, k1, b1 ));
         s1 = AveragePool( c1, [2,2]);
         c2 = Logistic( Conv( s1, k2, b2));
         s2 = AveragePool( c2, [2,2]);
         out = Logistic( Conv( s2, fc, b));

         d_out = out - target;

         print( out);
         printf( "The error is %f\n", tod( MeanSquaredError( out, target)));

         // d_s2, fc, b = d_out |> BackLogistic( out) |> BackConv( fc, s2, b, rate);
         // d_s1, k2, b2 = d_s2 |> BackAveragePool( [2,2]) |> BackLogistic( c2) |> BackConv( k2, s1, b2, rate);
         // _, k1, b1 = d_s1    |> BackAveragePool( [2,2]) |> BackLogistic( c1) |> BackConv( k1, in, b1, rate);

         d_s2, fc, b = BackConv( BackLogistic( d_out, out), fc, s2, b, rate);
         d_c2 = BackAveragePool( d_s2, [2,2]);
         d_s1, k2, b2 = BackConv( BackLogistic( d_c2, c2), k2, s1, b2, rate);
         d_c1 = BackAveragePool( d_s1, [2,2]);
         _, k1, b1 = BackConv( BackLogistic( d_c1, c1), k1, in, b1, rate);

      }

   }

}
  
//------------------------------------------------------------------------------

void RunMazur()
{
  w14 = [[0.15f, 0.25f], [0.2f, 0.3f]];
  b1 = [0.35f, 0.35f];
  w58 = [[[0.4f, 0.5f], [0.45f, 0.55f]]];
  b2 = [0.6f, 0.6f];

  in = [0.05f, 0.1f];
  target = [[[0.01f, 0.99f]]];
  epocs = 10000;
  epocs_output = 1000;
  rate = 0.5f;

  printf( "Running Mazur with rate %f\n", tod( rate));

  for( i=0; i<epocs; i++) {
    // forward sweep:
    h = Logistic( Conv( in, w14, b1 ));
    o = Logistic( Conv( h, w58, b2 ));
   
    d_o = o - target;

    if( i % epocs_output == 0) {
      print( o);
      printf( "The error after %d epochs is %f\n", i, tod( MeanSquaredError( o, target)));
    }

    // back propagation:
    d_h, w58, b2 = BackConv( BackLogistic( d_o, o), w58, h, b2, rate);
    _, w14, b1 = BackConv( BackLogistic( d_h, h), w14, in, b1, rate);
  }

  h = Logistic( Conv( in, w14, b1 ));
  o = Logistic( Conv( h, w58, b2 ));
  print( o);
  printf( "The error after %d epochs is %f\n", i, tod( MeanSquaredError( o, target)));

}

//------------------------------------------------------------------------------

int main()
{
   RunZhang();

   return 0;
}

