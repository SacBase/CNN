use MathArray: all;
use Array: all except {sum};
import Array: {sum};
use StdIO: all;

inline
float[*] sum( int outer, float[*] array)
{
  return with {
           ( take( [outer], 0*shape(array)) <= iv < take( [outer], shape( array))): array[iv];
         } : fold( +, genarray( drop( [outer], shape( array)), 0f));
}

inline
float average( float[*] array)
{
   return sum( array) / tof( prod( shape( array)));
}

inline StripOnes( int[.] shp)
{
   num_ones = sum( toi( shp == 1));
   res = genarray( shp - num_ones, 0);
   idx = 0;
   for( i=0; i< shp[0]; i++) {
      if( shp[i] !=1) {
        shp[idx] = shp[i];
        idx++;
      }
   }
   return shp;
}

//------------------------------------------------------------------------------
// Convolution function
//------------------------------------------------------------------------------
//specialize float[*] Conv( float[10,10,1] in, float[3,3,1,3] weights, float[3] bias);

inline
float[*] Conv( float[*] in, float[*] weights, float[*] bias)
{
  shp_act_map = (shape(in) - take( [dim(in)], shape(weights))) + 1;
  shp_maps = drop( [dim(in)], shape(weights));  // should be identical to shape(bias) !
  /*
   * { iv -> sum( dim(in), { ov -> weights[ov] * in[iv+ov]
   *                             | ov < take( [dim(in)], shape(weights)) } ) + bias
   *       | iv < shp_act_map }
   */
  out = with {
          (. <= iv <= .) : sum( dim(in),
                                with {
                                  (. <= ov <= .) : weights[ov] * in[iv+ov];
                                } : genarray( take( [dim(in)], shape(weights)), genarray( shp_maps, 0f))) + bias;
        } : genarray( shp_act_map, genarray( shp_maps, 0f));
  
  return reshape( StripOnes( shape( out)), out);
}


//------------------------------------------------------------------------------
// Activation functions
//------------------------------------------------------------------------------

inline
float[*] ReLU( float[*] in)
{
  return max( 0f, in);
}

inline
float[*] Linear( float[*] in)
{
  return in;
}

inline
float[*] Logistic( float[*] in)
{
  return 1f/(1f + exp( -(in)));
}

inline
float[*] BackLogistic( float[*] d_out)
{
  return d_out * (1-d_out);
}

inline
float[*] HyperbolicTangent( float[*] in)
{
  return (exp( 2f*in) - 1f) / (exp( 2f*in) + 1f);
}

//------------------------------------------------------------------------------
// Pooling functions
//------------------------------------------------------------------------------


inline
float[*] MaxPool( float[*] in, int[.] filter, int[.] stride)
{
  ones = genarray( [dim( in)], 1);
  filter = filter ++ drop( shape( filter), ones);
  stride = stride ++ drop( shape( stride), filter);
  shp = (shape( in) - filter) / stride + 1;
  /*
   * out = { iv -> maxval( { ov -> in[iv*stride+ov]
   *                             | ov < filter})
   *             | iv < shp};
   */
  out = with {
          (. <= iv <= .) : maxval( with {
                                     (. <= ov <= .) : in[iv*stride+ov];
                                   } : genarray( filter, 0f));
        } : genarray( shp, 0f);
  return out;
}

inline
float[*] AveragePool( float[*] in, int[.] filter, int[.] stride)
{
  ones = genarray( [dim( in)], 1);
  filter = filter ++ drop( shape( filter), ones);
  stride = stride ++ drop( shape( stride), filter);
  shp = (shape( in) - filter) / stride + 1;
  /*
   * out = { iv -> maxval( { ov -> in[iv*stride+ov]
   *                             | ov < filter})
   *             | iv < shp};
   */
  out = with {
          (. <= iv <= .) : average( with {
                                      (. <= ov <= .) : in[iv*stride+ov];
                                    } : genarray( filter, 0f));
        } : genarray( shp, 0f);
  return out;
}

inline
float[*] BackAveragePool( float[*] d_out, int[.] filter )
{
  ones = genarray( [dim( d_out)], 1);
  filter = filter ++ drop( shape( filter), ones);
  shp = shape( d_out) * filter;
  d_in = with {
           (. <= iv <=.) : d_out[iv/filter] / prod( filter);
         } : genarray( shp, 0f);
  return d_in;
}

//------------------------------------------------------------------------------
#if 0

float MeanSquaredError( float[.] result, float[.] labels)
{
  return sum ( 0.5f * ( labels - result) * ( labels - result) );
}

specialize float[*] BackConv( float[8,8,1] d_output, float[3,3,1] weights, float[10,10,1] input);
float[*] BackConv( float[*] d_output, float[*] weights, float[*] input)
{
  d_weights = with {
                ( . <= ov <= .) : sum( { iv -> input[ iv+ov] * d_output[iv]});
              } : genarray( shape( weights), 0f);
  return d_weights;
}

specialize float[*], float[*] BackConv2( float[8,8,1] d_output, float[3,3,1] weights, float[10,10,1] input);

float[*], float[*] BackConv2( float[*] d_output, float[*] weights, float[*] input)
{
  d_input = with {
              ( 0*shape( weights) <= ov < shape( weights)) :
                   with {
                    (ov <= iv < ov+shape(d_output)) : weights[ ov] * d_output[iv-ov];
                   } : genarray( shape( input), 0f );
            } : fold( +, genarray( shape(input), 0f));
  d_weights = with {
                ( 0*shape( d_output) <= iv < shape( d_output)) : 
                   with { 
                    (. <= ov <= .) : input[ iv+ov] * d_output[iv];
                   } : genarray( shape( weights), 0f );
              } : fold( +, genarray( shape(weights), 0f));
  return (d_input, d_weights);
}

#endif
//------------------------------------------------------------------------------
// Network Construction
//------------------------------------------------------------------------------

float[*], float[*], float[*]
Forward( float[28,28] in, float[5,5,6] k1, float[6] b1,
                          float[5,5,6,12] k2, float[12] b2,
                          float[4,4,12,10] fc, float[10] b)
{
   // s1 = in |> Conv( k1, b1) |> Logistic() |> AveragePool( [2,2]);
   // s2 = s1 |> Conv( k2, b2) |> Logistic() |> AveragePool( [2,2]);
   // out = s2 |> Conv( fc, b) |> Logistic() |> reshape( [10]);

   c1 = Logistic( Conv( in, k1, b1 ));
   s1 = AveragePool( c1, [2,2]);
   c2 = Logistic( Conv( s1, k2, b2));
   s2 = AveragePool( c2, [2,2]);
   out = Logistic( Conv( s2, fc, b));

   return (s1,s2,out);
}

float[*], float[*], float[*]
BackProp( float[10] d_out, float[4,4,1,12,10] fc,
          float[8,8,1,12] s2, float[5,5,6,12] k2,
          float[12,12,6] s1, float[5,5,6] k1, float[28,28] in)
{
   // s2, fc = d_out |> BackLogistic() |> BackConv( s2, fc);
   // s1, k2 = s2 |> BackAveragePool( [2,2]) |> BackLogistic() |> BackConv( s1, k2);
   // __, k1 = s1 |> BackAveragePool( [2,2]) |> BackLogistic() |> BackConv( in, k1);

   s2, fc = BackConv( BackLogistic( d_out), s2, fc);
   c2 = BackAveragePool( s2, [2,2]);
   s1, k2 = BackConv( BackLogistic( c2), s1, k2);
   c1 = BackAveragePool( s1, [2,2]);
   _, k1 = BackConv( BackLogistic( c1), in, k1);
   return (k1, k2, fc);
}

//------------------------------------------------------------------------------

float[*] GetTrainingInput( int i)
{
   in = genarray( [28,28], 0f);
   in[6,6] = 42f;
   return in;
}

//------------------------------------------------------------------------------

int main()
{

   epocs = 10;
   num_train_ins = 1;
   num_test_ins = 0;
   k1 = genarray( [5,5,6], 1f/25f);
   k2 = genarray( [5,5,6,12], 1f/150f);
   fc = genarray( [4,4,1,12,10], 1f/192f);

   for( epoc = 1; epoc <epocs; epoc++) {
      for( i=0; i<num_train_ins; i++) { 
         in = GetTrainingInput( i);
         target = GetTargetoutput( i);
         s1,s2,out = Forward( in, k1, k2, fc);
         d_out = out - target;
         k1,k2,fc = BackProp( d_out, fc, s2,k2, s1, k1, in);
      }
   }

   for( i=0; i<num_test_ins; i++) {
      in = GetTrainingInput( i);
      _,__,out = Forward( in, k1, k2, fc);
      print( out);
   }
  

   return 0;
}

